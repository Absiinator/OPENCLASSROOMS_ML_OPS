"""
API FastAPI pour le scoring cr√©dit Home Credit.
================================================

Cette API expose le mod√®le de scoring cr√©dit pour:
- Pr√©diction individuelle
- Pr√©diction batch
- Explication des pr√©dictions
- Informations sur le mod√®le

D√©ployable sur Render, Railway ou tout cloud provider.
"""

import os
import json
import sys
from pathlib import Path
from typing import Optional, List, Dict, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import pandas as pd
import numpy as np

# Ajouter le chemin parent pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from api.models import (
    ClientFeatures,
    PredictionResponse,
    BatchPredictionRequest,
    BatchPredictionResponse,
    ExplanationResponse,
    FeatureContribution,
    FeatureImportance,
    ModelInfo,
    HealthResponse,
    ErrorResponse,
    RiskCategory,
    Decision
)

# Configuration
API_VERSION = "1.0.0"
MODEL_DIR = Path(__file__).parent.parent / "models"

# Variables globales pour le mod√®le
model = None
preprocessor = None
config = None


def get_risk_category(probability: float) -> RiskCategory:
    """D√©termine la cat√©gorie de risque selon la probabilit√©."""
    if probability < 0.2:
        return RiskCategory.VERY_LOW
    elif probability < 0.4:
        return RiskCategory.LOW
    elif probability < 0.6:
        return RiskCategory.MODERATE
    elif probability < 0.8:
        return RiskCategory.HIGH
    else:
        return RiskCategory.VERY_HIGH


def load_model_artifacts():
    """Charge le mod√®le et le pr√©processeur."""
    global model, preprocessor, config
    
    import joblib
    
    model_path = MODEL_DIR / "lgbm_model.joblib"
    preprocessor_path = MODEL_DIR / "preprocessor.joblib"
    config_path = MODEL_DIR / "model_config.json"
    
    # V√©rifier si les fichiers existent
    if not model_path.exists():
        raise FileNotFoundError(f"Mod√®le non trouv√©: {model_path}")
    if not preprocessor_path.exists():
        raise FileNotFoundError(f"Pr√©processeur non trouv√©: {preprocessor_path}")
    
    # Charger le mod√®le
    model = joblib.load(model_path)
    print(f"‚úÖ Mod√®le charg√©: {model_path}")
    
    # Charger le pr√©processeur
    preprocessor = joblib.load(preprocessor_path)
    print(f"‚úÖ Pr√©processeur charg√©: {preprocessor_path}")
    
    # Charger la configuration
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
        print(f"‚úÖ Configuration charg√©e: seuil={config.get('optimal_threshold', 0.5):.3f}")
    else:
        config = {"optimal_threshold": 0.5, "cost_fn": 10, "cost_fp": 1}
        print("‚ö†Ô∏è Configuration par d√©faut utilis√©e")
    
    return True


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestionnaire du cycle de vie de l'application."""
    # Startup
    try:
        load_model_artifacts()
        print("üöÄ API d√©marr√©e avec succ√®s!")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur au chargement: {e}")
        print("   L'API d√©marre mais les pr√©dictions ne fonctionneront pas.")
    
    yield
    
    # Shutdown
    print("üëã API arr√™t√©e")


# Cr√©er l'application FastAPI
app = FastAPI(
    title="Home Credit Scoring API",
    description="""
    API de scoring cr√©dit pour le projet Home Credit.
    
    ## Fonctionnalit√©s
    
    * **Pr√©diction** : Obtenir la probabilit√© de d√©faut d'un client
    * **Batch** : Pr√©dire pour plusieurs clients en une requ√™te
    * **Explication** : Comprendre les facteurs de risque
    * **Info** : Informations sur le mod√®le d√©ploy√©
    
    ## Co√ªt m√©tier
    
    Le mod√®le est optimis√© pour minimiser le co√ªt m√©tier:
    - Faux N√©gatif (d√©faut non d√©tect√©) : co√ªt = 10
    - Faux Positif (bon client refus√©) : co√ªt = 1
    
    ## Seuil de d√©cision
    
    Le seuil optimal est d√©termin√© lors de l'entra√Ænement pour minimiser le co√ªt total.
    """,
    version=API_VERSION,
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, restreindre aux origines autoris√©es
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/", tags=["Root"])
async def root():
    """Point d'entr√©e de l'API."""
    return {
        "message": "Home Credit Scoring API",
        "version": API_VERSION,
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """V√©rification de l'√©tat de l'API."""
    return HealthResponse(
        status="healthy" if model is not None else "degraded",
        model_loaded=model is not None,
        version=API_VERSION
    )


@app.get("/model/info", response_model=ModelInfo, tags=["Model"])
async def get_model_info():
    """Obtenir les informations sur le mod√®le d√©ploy√©."""
    if model is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    return ModelInfo(
        model_name=config.get("model_name", "home_credit_model"),
        version=API_VERSION,
        optimal_threshold=config.get("optimal_threshold", 0.5),
        cost_fn=config.get("cost_fn", 10),
        cost_fp=config.get("cost_fp", 1),
        n_features=len(preprocessor.feature_names) if preprocessor else 0,
        training_date=config.get("training_date")
    )


@app.post("/predict", response_model=PredictionResponse, tags=["Prediction"])
async def predict(
    client: ClientFeatures,
    threshold: Optional[float] = Query(None, ge=0, le=1, description="Seuil personnalis√©")
):
    """
    Pr√©dit la probabilit√© de d√©faut pour un client.
    
    - **client**: Donn√©es du client
    - **threshold**: Seuil de d√©cision personnalis√© (optionnel, utilise l'optimal par d√©faut)
    
    Retourne la probabilit√©, la d√©cision et la cat√©gorie de risque.
    """
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    try:
        # Convertir en DataFrame
        client_dict = client.model_dump(exclude_none=True)
        df = pd.DataFrame([client_dict])
        
        # Pr√©traitement
        X = preprocessor.transform(df)
        
        # Pr√©diction
        probability = float(model.predict_proba(X)[0, 1])
        
        # Seuil
        used_threshold = threshold if threshold is not None else config.get("optimal_threshold", 0.5)
        prediction = 1 if probability >= used_threshold else 0
        
        return PredictionResponse(
            client_id=client.SK_ID_CURR,
            probability=round(probability, 4),
            prediction=prediction,
            decision=Decision.REFUSED if prediction == 1 else Decision.ACCEPTED,
            risk_category=get_risk_category(probability),
            threshold=used_threshold
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur de pr√©diction: {str(e)}")


@app.post("/predict/batch", response_model=BatchPredictionResponse, tags=["Prediction"])
async def predict_batch(request: BatchPredictionRequest):
    """
    Pr√©dit la probabilit√© de d√©faut pour plusieurs clients.
    
    - **clients**: Liste des donn√©es clients
    - **threshold**: Seuil personnalis√© (optionnel)
    
    Retourne les pr√©dictions pour chaque client et un r√©sum√©.
    """
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    if len(request.clients) == 0:
        raise HTTPException(status_code=400, detail="Liste de clients vide")
    
    if len(request.clients) > 1000:
        raise HTTPException(status_code=400, detail="Maximum 1000 clients par requ√™te")
    
    try:
        # Convertir en DataFrame
        clients_data = [c.model_dump(exclude_none=True) for c in request.clients]
        df = pd.DataFrame(clients_data)
        
        # Pr√©traitement
        X = preprocessor.transform(df)
        
        # Pr√©dictions
        probabilities = model.predict_proba(X)[:, 1]
        
        # Seuil
        used_threshold = request.threshold if request.threshold is not None else config.get("optimal_threshold", 0.5)
        
        # Construire les r√©ponses
        predictions = []
        for i, (client, proba) in enumerate(zip(request.clients, probabilities)):
            prediction = 1 if proba >= used_threshold else 0
            predictions.append(PredictionResponse(
                client_id=client.SK_ID_CURR,
                probability=round(float(proba), 4),
                prediction=prediction,
                decision=Decision.REFUSED if prediction == 1 else Decision.ACCEPTED,
                risk_category=get_risk_category(proba),
                threshold=used_threshold
            ))
        
        accepted = sum(1 for p in predictions if p.prediction == 0)
        refused = len(predictions) - accepted
        
        return BatchPredictionResponse(
            predictions=predictions,
            total_clients=len(predictions),
            accepted_count=accepted,
            refused_count=refused
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur de pr√©diction batch: {str(e)}")


@app.post("/predict/explain", response_model=ExplanationResponse, tags=["Explanation"])
async def explain_prediction(client: ClientFeatures):
    """
    Explique la pr√©diction pour un client avec les features les plus influentes.
    
    Utilise l'importance des features du mod√®le pour identifier
    les facteurs qui contribuent le plus √† la d√©cision.
    """
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    try:
        # Convertir en DataFrame
        client_dict = client.model_dump(exclude_none=True)
        df = pd.DataFrame([client_dict])
        
        # Pr√©traitement
        X = preprocessor.transform(df)
        
        # Pr√©diction
        probability = float(model.predict_proba(X)[0, 1])
        threshold = config.get("optimal_threshold", 0.5)
        prediction = 1 if probability >= threshold else 0
        
        # Feature importance (globale pour simplifier)
        feature_importances = model.feature_importances_
        feature_names = preprocessor.feature_names
        
        # Top features par importance
        sorted_indices = np.argsort(feature_importances)[::-1][:10]
        
        top_features = []
        for idx in sorted_indices:
            feature_name = feature_names[idx]
            feature_value = float(X[0, idx]) if not np.isnan(X[0, idx]) else 0.0
            importance = float(feature_importances[idx])
            
            # D√©terminer la direction bas√©e sur la valeur et l'importance
            direction = "augmente le risque" if feature_value > 0 else "diminue le risque"
            
            top_features.append(FeatureContribution(
                feature=feature_name,
                value=round(feature_value, 4),
                contribution=round(importance, 4),
                direction=direction
            ))
        
        return ExplanationResponse(
            client_id=client.SK_ID_CURR,
            probability=round(probability, 4),
            prediction=prediction,
            decision=Decision.REFUSED if prediction == 1 else Decision.ACCEPTED,
            top_features=top_features
        )
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Erreur d'explication: {str(e)}")


@app.get("/model/features", response_model=List[FeatureImportance], tags=["Model"])
async def get_feature_importance(top_n: int = Query(20, ge=1, le=100)):
    """
    Obtenir l'importance des features du mod√®le.
    
    - **top_n**: Nombre de features √† retourner (1-100)
    """
    if model is None or preprocessor is None:
        raise HTTPException(status_code=503, detail="Mod√®le non charg√©")
    
    try:
        feature_importances = model.feature_importances_
        feature_names = preprocessor.feature_names
        
        # Trier par importance
        sorted_indices = np.argsort(feature_importances)[::-1][:top_n]
        
        result = []
        for rank, idx in enumerate(sorted_indices, 1):
            result.append(FeatureImportance(
                feature=feature_names[idx],
                importance=round(float(feature_importances[idx]), 4),
                rank=rank
            ))
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")


@app.get("/client/{client_id}", response_model=PredictionResponse, tags=["Client"])
async def get_client_prediction(
    client_id: int,
    threshold: Optional[float] = Query(None, ge=0, le=1)
):
    """
    Obtenir la pr√©diction pour un client par son ID.
    
    Note: Cette endpoint n√©cessite que les donn√©es du client soient disponibles
    dans la base de donn√©es ou le fichier de donn√©es.
    """
    # Cette impl√©mentation est un placeholder
    # En production, vous chargeriez les donn√©es du client depuis une base de donn√©es
    raise HTTPException(
        status_code=501, 
        detail="Endpoint non impl√©ment√©. Utilisez /predict avec les donn√©es du client."
    )


# Gestion des erreurs
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    return JSONResponse(
        status_code=500,
        content={"detail": f"Erreur interne: {str(exc)}"}
    )


if __name__ == "__main__":
    import uvicorn
    
    port = int(os.environ.get("PORT", 8000))
    host = os.environ.get("HOST", "0.0.0.0")
    
    uvicorn.run(
        "main:app",
        host=host,
        port=port,
        reload=True
    )
