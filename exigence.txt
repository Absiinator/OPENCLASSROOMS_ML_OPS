L’API de prédiction du score, déployée sur le cloud (lien vers l’API).
Le notebook ou code de la modélisation (du prétraitement à la prédiction)
Ce notebook intègre la partie MLFlow de génération du tracking d'expérimentations. 
L’interface web "UI MLFlow" d'affichage des résultats du tracking MLFlow sera présentée en soutenance + copie d’écran dans le support de soutenance.
Un dossier, géré via un outil de versioning de code contenant :
Le notebook ou code de la modélisation (du prétraitement à la prédiction), intégrant via MLFlow le tracking d’expérimentations et le stockage centralisé des modèles.
Le code permettant de déployer le modèle sous forme d'API.
Pour l’API, un fichier introductif permettant de comprendre l'objectif du projet et le découpage des dossiers, et un fichier listant les packages utilisés seront présents dans le dossier.
Le tableau HTML d’analyse de data drift réalisé à partir d’evidently.
Un notebook ou une application Streamlit de test de l’API.
Un support de présentation pour la soutenance, détaillant le travail réalisé (Powerpoint ou équivalent, 30 slides maximum), intégrant des copies écran, preuves qu’un pipeline de déploiement continu a permis de déployer l’API : 
de l’interface web 'UI MLFlow" d'affichage des résultats du tracking MLFlow ;
des commits ;
du dossier Github (+ lien vers ce dossier) ;
de l’exécution des tests unitaires ;
de l’exécution du déploiement de l’API avec lien vers l’API sur le Cloud.

Définir la stratégie d’élaboration d’un modèle d’apprentissage supervisé et sélectionner et entraîner des modèles adaptés à une problématique métier afin de réaliser une analyse prédictive.
CE1 Les variables catégorielles identifiées ont été transformées en fonction du besoin (par exemple via OneHotEncoder ou TargetEncoder). 

CE2 Vous avez a créé de nouvelles variables à partir de variables existantes. 

CE3 Vous avez réalisé des transformations mathématiques lorsque c'est requis pour transformer les distributions de variables. 

CE4 Vous avez normalisé les variables lorsque c'est requis. 

CE5 Vous avez défini sa stratégie d’élaboration d’un modèle pour répondre à un besoin métier. Cela signifie dans ce projet que : 

l’étudiant a présenté son approche méthodologique de modélisation dans son support de présentation pendant la soutenance et est capable de répondre à des questions à ce sujet, si elles lui sont posées.
CE6 Vous avez choisi la ou les variables cibles pertinentes.

CE7 Vous avez vérifié qu'il n’y a pas de problème de data leakage (c'est-à-dire, des variables trop corrélées à la variable cible et inconnues a priori dans les données en entrée du modèle).

CE8 Vous avez testé plusieurs algorithmes de façon cohérente, en partant des plus simples vers les plus complexes (au minimum un linéaire et un non linéaire).

 

Évaluer les performances des modèles d’apprentissage supervisé selon différents critères (scores, temps d'entraînement, etc.) en adaptant les paramètres afin de choisir le modèle le plus performant pour la problématique métier.

CE1 Vous avez choisi une métrique adaptée pour évaluer la performance d'un algorithme (par exemple : R2 ou RMSE en régression, accuracy ou AUC en classification, etc.). Dans le cadre de ce projet, cela signifie que :

Vous avez mis en oeuvre un score métier pour évaluer les modèles et optimiser les hyperparamètres, qui prend en compte les spécificités du contexte, en particulier le fait que le coût d’un faux négatif et d’un faux positif sont sensiblement différents.
CE2 Vous avez exploré d'autres indicateurs de performance que le score pour comprendre les résultats (coefficients des variables en fonction de la pénalisation, visualisation des erreurs en fonction des variables du modèle, temps de calcul...).

CE3 Vous avez séparé les données en train/test pour les évaluer de façon pertinente et détecter l'overfitting.

CE4 Vous avez mis en place un modèle simple de référence pour évaluer le pouvoir prédictif du modèle choisi (dummyRegressor ou dummyClassifier).

CE5 Vous avez pris en compte dans sa démarche de modélisation l'éventuel déséquilibre des classes (dans le cas d'une classification).

CE6 Vous avez optimisé les hyper-paramètres pertinents dans les différents algorithmes.

CE7 Vous avez mis en place une validation croisée (via GridsearchCV, RandomizedSearchCV ou équivalent) afin d’optimiser les hyperparamètres et comparer les modèles. Dans le cadre de ce projet : 

une cross-validation du dataset train est réalisée ;
un premier test de différentes valeurs d’hyperparamètres est réalisé sur chaque algorithme testé, et affiné pour l’algorithme final choisi ;
tout projet présentant un score AUC anormalement élevé, démontrant de l’overfitting dans le GrisSearchCV, sera invalidé (il ne devrait pas être supérieur au meilleur de la compétition Kaggle : 0.82).
CE8 Vous avez présenté l'ensemble des résultats en allant des modèles les plus simples aux plus complexes. Vous avez justifié le choix final de l'algorithme et des hyperparamètres.

CE9 Vous avez réalisé l’analyse de l’importance des variables (feature importance) globale sur l’ensemble du jeu de données et locale sur chaque individu du jeu de données.

 

Définir et mettre en œuvre un pipeline d’entraînement des modèles, avec centralisation du stockage des modèles et formalisation des résultats et mesures des différentes expérimentations réalisées, afin d’industrialiser le projet de Machine Learning.
CE1 Vous avez mis en oeuvre un pipeline d’entraînement des modèles reproductible.

CE2 Vous avez sérialisé et stocké les modèles créés dans un registre centralisé afin de pouvoir facilement les réutiliser.

CE3 Vous avez formalisé des mesures et résultats de chaque expérimentation, afin de les analyser et de les comparer

 

Mettre en œuvre un logiciel de version de code afin d’assurer en continu l’intégration et la diffusion du modèle auprès de collaborateurs.
CE1 Vous avez créé un dossier contenant tous les scripts du projet dans un logiciel de version de code avec Git et l'a partagé avec Github.

CE2 Vous avez présenté un historique des modifications du projet qui affiche au moins trois versions distinctes, auxquelles il est possible d'accéder.

CE3 Vous avez tenu à jour et mis à disposition la liste des packages utilisés ainsi que leur numéro de version.

CE4 Vous avez rédigé un fichier introductif permettant de comprendre l'objectif du projet et le découpage des dossiers.

CE5 Vous avez commenté les scripts et les fonctions facilitant une réutilisation du travail par d'autres personnes et la collaboration.

 

Concevoir et assurer un déploiement continu d'un moteur d’inférence (modèle de prédiction encapsulé dans une API) sur une plateforme Cloud afin de permettre à des applications de réaliser des prédictions via une requête à l’API.
CE1 Vous avez défini et préparé un pipeline de déploiement continu.

CE2 Vous avez déployé le modèle de machine learning sous forme d'API (via Flask par exemple) et cette API renvoie bien une prédiction correspondant à une demande.

CE3 Vous avez mis en œuvre un pipeline de déploiement continu, afin de déployer l'API sur un serveur d'une plateforme Cloud.

CE4 Vous avez mis en oeuvre des tests unitaires automatisés (par exemple avec pyTest).

CE5 Vous avez réalisé l'API indépendamment de l'application qui utilise le résultat de la prédiction.

 

Définir et mettre en œuvre une stratégie de suivi de la performance d’un modèle en production et en assurer la maintenance afin de garantir dans le temps la production de prédictions performantes.
CE1 Vous avez défini une stratégie de suivi de la performance du modèle. Dans le cadre du projet : 

choix de réaliser a priori cette analyse sur le dataset disponible : analyse de data drift entre le dataset train et le dataset test.
CE2 Vous avez réalisé un système de stockage d’événements relatifs aux prédictions réalisées par l’API et une gestion d’alerte en cas de dégradation significative de la performance. Dans le cadre du projet : 

choix de réaliser a priori cette analyse analyse de data drift, via une simulation dans un notebook et création d’un tableau HTML d’analyse avec la librairie evidently.
CE3 Vous avez analysé la stabilité du modèle dans le temps et défini des actions d’amélioration de sa performance. Dans le cadre de ce projet : 

analyse du tableau HTML evidently, et conclusion sur un éventuel data drift.

L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un algorithme de classification en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.)

Voici les données dont vous aurez besoin pour réaliser l’algorithme de classification. Pour plus de simplicité, vous pouvez les télécharger à cette adresse.

Vous aurez besoin de joindre les différentes tables entre elles.

Votre mission :

Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.

Analyser les features qui contribuent le plus au modèle, d’une manière générale (feature importance globale) et au niveau d’un client (feature importance locale), afin, dans un soucis de transparence, de permettre à un chargé d’études de mieux comprendre le score attribué par le modèle.

Mettre en production le modèle de scoring de prédiction à l’aide d’une API et réaliser une interface de test de cette API.

Mettre en œuvre une approche globale MLOps de bout en bout, du tracking des expérimentations à l’analyse en production du data drift.

Michaël, votre manager, vous incite à sélectionner un ou des kernels Kaggle pour vous faciliter l’analyse exploratoire, la préparation des données et le feature engineering nécessaires à l’élaboration du modèle de scoring. 

Si vous le faites, vous devez analyser ce ou ces kernels et le ou les adapter pour vous assurer qu’il(s) répond(ent) aux besoins de votre mission.

Par exemple vous pouvez vous inspirer des kernels suivants : 

Pour l’analyse exploratoire
Pour la préparation des données et le feature engineering 
C’est optionnel, mais nous vous encourageons à le faire afin de vous permettre de vous focaliser sur l’élaboration du modèle, son optimisation et sa compréhension.

De : Mickael

À : moi

Objet : Besoins et conseils pour l’élaboration d’un outil de credit scoring 

Bonjour,

 

Afin de pouvoir faire évoluer régulièrement le modèle, je souhaite tester la mise en œuvre une démarche de type MLOps d’automatisation et d’industrialisation de la gestion du cycle de vie du modèle. 

 

Vous trouverez en pièce jointe la liste d’outils à utiliser pour créer une plateforme MLOps qui s’appuie sur des outils Open Source. 

 

Je souhaite que vous puissiez mettre en oeuvre au minimum les étapes orientées MLOps suivantes : 

Dans le notebook d’entraînement des modèles, générer à l’aide de MLFlow un tracking d'expérimentations
Lancer l’interface web 'UI MLFlow" d'affichage des résultats du tracking
Réaliser avec MLFlow un stockage centralisé des modèles dans un “model registry”
Tester le serving MLFlow
Gérer le code avec le logiciel de version Git
Partager le code sur Github pour assurer une intégration continue
Utiliser Github Actions pour le déploiement continu et automatisé du code de l’API sur le cloud
Concevoir des tests unitaires avec Pytest (ou Unittest) et les exécuter de manière automatisée lors du build réalisé par Github Actions
 

J’ai également rassemblé des conseils pour vous aider à vous lancer dans ce projet !

 

Concernant l’élaboration du modèle soyez vigilant sur deux points spécifiques au contexte métier : 

Le déséquilibre entre le nombre de bons et de moins bons clients doit être pris en compte pour élaborer un modèle pertinent, avec une méthode au choix
Le déséquilibre du coût métier entre un faux négatif (FN - mauvais client prédit bon client : donc crédit accordé et perte en capital) et un faux positif (FP - bon client prédit mauvais : donc refus crédit et manque à gagner en marge)
Vous pourrez supposer, par exemple, que le coût d’un FN est dix fois supérieur au coût d’un FP
Vous créerez un score “métier” (minimisation du coût d’erreur de prédiction des FN et FP) pour comparer les modèles, afin de choisir le meilleur modèle et ses meilleurs hyperparamètres. Attention cette minimisation du coût métier doit passer par l’optimisation du seuil qui détermine, à partir d’une probabilité, la classe 0 ou 1 (un “predict” suppose un seuil à 0.5 qui n’est pas forcément l’optimum)
En parallèle, maintenez pour comparaison et contrôle des mesures plus techniques, telles que l’AUC et l’accuracy 
 

D’autre part je souhaite que vous mettiez en œuvre une démarche d’élaboration des modèles avec Cross-Validation et optimisation des hyperparamètres, via GridsearchCV ou équivalent.

 

Un dernier conseil : si vous obtenez des scores supérieurs au 1er du challenge Kaggle (AUC > 0.82), posez-vous la question si vous n’avez pas de l’overfitting dans votre modèle !

 

Vous exposerez votre modèle de prédiction sous forme d’une API qui permet de calculer la probabilité de défaut du client, ainsi que sa classe (accepté ou refusé) en fonction du seuil optimisé d’un point de vue métier.

 

Le déploiement de l’API sera réalisée sur une plateforme Cloud, de préférence une solution gratuite.

 

Je vous propose d’utiliser un Notebook ou une application Streamlit pour réaliser en local  l’interface de test de l’API.

 

Bon courage !

 

Mickael

Barres titres
Vous continuez votre mission au sein de "Prêt à dépenser". Deux semaines après votre dernière échange sur l'outil scoring, vous avez bien avancé et Mickael vous envoie un nouveau mail.

 

De : Mickael

À : moi

Objet : Compléments MLOps

Bonjour,

 

Comme vous avez pu le constater le cycle de vie du modèle n’est pas complet, j’ai oublié dans la démarche MLOps la dernière étape de suivi de la performance du modèle en production. C’est un peu normal car le modèle n’est pas encore en production !

 

En prévision, je souhaiterais que vous testiez l’utilisation de la librairie evidently pour détecter dans le futur du Data Drift en production. Pour cela vous prendrez comme hypothèse que le dataset “application_train” représente les datas pour la modélisation et le dataset “application_test” représente les datas de nouveaux clients une fois le modèle en production. 

 

L’analyse à l’aide d’evidently vous permettra de détecter éventuellement du Data Drift sur les principales features, entre les datas d’entraînement et les datas de production, au travers du tableau HTML d’analyse que vous aurez généré grâce à evidently.

 

P.S.: Je remets en pièce jointe la même liste d’outils MLOps que je vous ai partagée précédemment.

 

Merci par avance !

 

Mickael

Barres titres
 

Vous êtes Data Scientist au sein d'une société financière nommée Prêt à dépenser, qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.

 



 

Vous venez de mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, et classifier la demande en crédit accordé ou refusé. 

 

Les chargés de relation client ont fait remonter le fait que les clients sont de plus en plus demandeurs de transparence vis-à-vis des décisions d’octroi de crédit. 

 

Prêt à dépenser décide donc de développer un dashboard interactif pour que les chargés de relation client puissent expliquer de façon la plus transparente possible les décisions d’octroi de crédit, lors de rendez-vous avec eux. Cette volonté de transparence va tout à fait dans le sens des valeurs que l’entreprise veut incarner.

 

Vous reprendrez les mêmes données que vous avez utilisées pour réaliser le modèle de classification. Vous pouvez les télécharger directement à cette adresse.

 

Michaël vous a envoyé un mail pour vous préciser ses attentes et ses conseils.

 

De : Michaël 

À : moi

Objet : Scoring crédit

Bonjour,

 

Bravo pour la réalisation de l’outil de scoring et ta présentation à l’équipe. Tu as assuré !

 

Maintenant nous souhaitons utiliser l’API pour réaliser un dashboard à destination de nos chargés de relation client. Ils le réclament depuis longtemps afin de pouvoir mieux expliquer à leurs clients les décisions, et parfois les revoir si nécessaire.

 

Voici les spécifications pour le dashboard interactif :

Permettre de visualiser le score, sa probabilité (est-il loin du seuil ?) et l’interprétation de ce score pour chaque client de façon intelligible pour une personne non experte en data science.
Permettre de visualiser les principales informations descriptives relatives à un client.
Permettre de comparer, à l’aide de graphiques, les principales informations descriptives relatives à un client à l’ensemble des clients ou à un groupe de clients similaires (via un système de filtre : par exemple, liste déroulante des principales variables).
Prendre en compte le besoin des personnes en situation de handicap dans la réalisation des graphiques, en couvrant des critères d'accessibilité du WCAG.
Déployer le dashboard sur une plateforme Cloud, afin qu'il soit accessible pour d'autres utilisateurs sur leur poste de travail.
Optionnellement (si tu as le temps) : Permettre d’obtenir un score et une probabilité rafraîchis après avoir saisi une modification d’une ou plusieurs informations relatives à un client, ainsi que de saisir un nouveau dossier client pour en obtenir le score et la probabilité.
 

C’est une première itération que nous présenterons aux chargés de relation client pour nous assurer que nous répondons à leur besoin. 

 

Je te propose d’utiliser Dash ou Bokeh ou Streamlit pour réaliser ce dashboard interactif. Ce dashboard appellera l’API de prédiction que tu as déjà réalisée pour déterminer la probabilité et la classe (accord ou refus) d’un client. 

 

Bon courage !

Michaël 

Réaliser un tableau de bord afin de présenter son travail de modélisation à un public non technique et de manière adaptée à des personnes en situation de handicap en appliquant certains critères d'accessibilité du WCAG
CE1 Vous avez décrit et conçu un parcours utilisateur simple permettant de répondre aux besoins des utilisateurs (les différentes actions et clics sur les différents graphiques permettent de répondre à une question que se pose l'utilisateur). 
CE2 Vous avez développé au moins deux graphiques interactifs permettant aux utilisateurs d'explorer les données. 
CE3 Vous avez réalisé des graphiques lisibles (taille de texte suffisante, définition lisible). 
CE4 Vous avez réalisé des graphiques qui permettent de répondre à la problématique métier. 
CE5 Vous avez pris en compte le besoin des personnes en situation de handicap dans la réalisation des graphiques : vous avez pris en compte au minimum les critères d'accessibilité du WCAG suivants : 
Critère de succès 1.1.1 Contenu non textuel 
Critère de succès 1.4.1 Utilisation de la couleur 
Critère de succès 1.4.3 Contraste (minimum) 
Critère de succès 1.4.4 Redimensionnement du texte 
Critère de succès 2.4.2 Titre de page 
CE6 Vous avez déployé le dashboard sur le web afin qu'il soit accessible pour d'autres utilisateurs sur leurs postes de travail.

Réaliser une veille sur les outils et tendances en data science et IA afin de mettre à jour son expertise et de s’assurer que les méthodes utilisées mobilisent bien les techniques en vigueur 
CE1 Vous avez consulté des sources reconnues d'informations, produites récemment (blogs reconnus, articles de recherche de journaux et conférences reconnues dans le domaine). 
CE2 Vous avez présenté les points clés de chacune des sources bibliographiques, y compris des détails mathématiques.
CE3 Vous avez mis en place une preuve de concept pour tester le nouvel outil / le nouveau modèle, la nouvelle démarche, et le comparer à une approche classique. 

Rédiger une note méthodologique contenant notamment le choix des algorithmes testés, les métriques utilisées et l’interprétabilité du modèle proposé, afin de communiquer sa démarche de modélisation
Dans le cadre de ce projet, cette note concerne la démarche de modélisation du nouveau modèle suite au travail de veille.

CE1 Vous avez présenté la démarche de modélisation de manière synthétique dans une note. 
CE2 Vous avez explicité la métrique d'évaluation retenue et votre démarche d'optimisation. 
CE3 Vous avez explicité l'interprétabilité globale et locale du modèle.
CE4 Vous avez décrit les limites et les améliorations envisageables pour gagner en performance et en interprétabilité de l'approche de modélisation. 
 

Réaliser la présentation orale d’une démarche de modélisation à un client interne/externe afin de partager les résultats et faciliter la prise de décision de l'interlocuteur
Dans le cadre de ce projet, cette présentation orale (soutenance) concerne l’ensemble du projet, l’élaboration du dashboard et la démarche de modélisation du nouveau modèle suite au travail de veille.

CE1 Vous avez expliqué de manière compréhensible par un public non technique la méthode d'évaluation de la performance du modèle de machine learning, la façon d'interpréter les résultats du modèle, et la façon d'interpréter l'importance des variables du modèle. 
CE2 Vous avez su répondre de manière simple (compréhensible par un public non technique) à au moins une question portant sur sa démarche de modélisation.
CE3 Vous avez présenté une démarche de modélisation et une évaluation complète des modèles, en particulier la comparaison de plusieurs modèles.

